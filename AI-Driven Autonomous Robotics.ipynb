{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Autonomous Robotics: Human Tracking and Reactive Behaviour Analysis using AI and Sensing Technologies\n",
    "\n",
    "\n",
    "### Aims\n",
    "This project aims to design and implement a program in Python to control mobile robots which will guide a robot on how to effectively manoeuvre its way through an environment while tracking an instance of a human or multiple humans using the pre-trained [ssd_mobilenet_v2_coco](https://docs.openvino.ai/2021.4/omz_models_model_ssd_mobilenet_v2_coco.html) for object detection.\n",
    "Additionally, the project seeks to implement several robot reactive behaviours and high level behaviours.\n",
    "\n",
    "### Main tasks:\n",
    "\n",
    "* Part A: Reactive behaviours -- The robot must demonstrate the following four Braitenberg behaviours: Aggression, Fear, Love, and Curiousity.   \n",
    "\n",
    "* Part B: Following behaviour -- The robot follows a person whilst maintaining a safe distance. The robot should avoid collisions with the object's in the environment and the person it's tracking. In the event where there is more than one human visible to the robot, the robot is capable of detecting all humans visible in the frame but will only track and follow the movements of the human which is closest to the camera by calculating the distance of all visible humans and prioritising that which is closest.     \n",
    "\n",
    "\n",
    "### Objectives:\n",
    "* Design and implement a program in Python which controls the movements of a robot.\n",
    "* The robot to adhere to the four Braitenberg behaviours. \n",
    "* The robot to detect a human using the pretrained model. \n",
    "* The robot to track and follow a humanâ€™s movements whilst maintaining a safe distance. \n",
    "* The robot to avoid collisions with humans and other obstacles.  \n",
    "* The robot to track and follow only one human at an instance.\n",
    "\n",
    "**Further Information:**   \n",
    "\n",
    "This project will use mobile robots built with the latest AI and sensing technology. The robots have a latest Realsense camera from Intel, a Nano board from Nvidia to support image processing and deep learning etc, proximity sensors, IMU, motors and driving circuits.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 load the pre-trained object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "    \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    \n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "            self.depth_image_ = depth_image\n",
    "    \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create helper module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Helper Module---\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def create_blue_box(image, bbox):\n",
    "    cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 2)    \n",
    "    \n",
    "def get_x_y(bbox):\n",
    "    # ymin, xmin, ymax, xmax\n",
    "    y_min, x_min, y_max, x_max = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    middle_width = round(y_min + (y_max - y_min)/2, 1)\n",
    "    middle_height = round(x_min + (x_max - x_min)/2, 1)\n",
    "    y = width * middle_width\n",
    "    x = height * middle_height\n",
    "    return (x, y, middle_width, middle_height)\n",
    "    \n",
    "def get_depth_from_data(depth_array, x, y):\n",
    "    return depth_array[int(x)-1, int(y)-1]\n",
    "    \n",
    "def get_closest_object(object_list):\n",
    "    depth_track = []\n",
    "    if len(object_list) > 1:\n",
    "        for object_ in object_list:\n",
    "            bbox = object_['bbox']\n",
    "            xy = get_x_y(bbox)\n",
    "            x, y = xy[0], xy[1]\n",
    "            detection_depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "            depth_track.append(detection_depth)\n",
    "        return get_closest(object_list, depth_track)\n",
    "    elif object_list:\n",
    "        return object_list[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_closest(object_list, depth_list):\n",
    "    if depth_list:\n",
    "        index = depth_list.index(min(depth_list))\n",
    "        return object_list[index]  \n",
    "        \n",
    "def obstacle_avoid_right():\n",
    "    robot.right(0.48)\n",
    "    time.sleep(1)\n",
    "    robot.forward(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.left(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.forward(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.stop()\n",
    "    \n",
    "def obstacle_avoid_left():\n",
    "    robot.left(0.48)\n",
    "    time.sleep(1)\n",
    "    robot.forward(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.right(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.forward(0.5)\n",
    "    time.sleep(1)\n",
    "    robot.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FOLLOWING AND AVOIDING OBSTACLES---\n",
    "def processing(change):\n",
    "    \n",
    "    depth_track = []\n",
    "    \n",
    "    image = change['new']8888\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    \n",
    "    depth_data = camera.depth_image_\n",
    "    depth_min = np.amin(depth_data[depth_data != 0])\n",
    "    if 0 < depth_min < 160:\n",
    "        robot.backward(2)\n",
    "        robot.left(0.5)\n",
    "        robot.stop()\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    #\n",
    "    \n",
    "    #   AVOID BOTTLES  \n",
    "    matching_detections = []\n",
    "    obstacle = []\n",
    "    \n",
    "    for d in detections[0]:\n",
    "        label = d['label']\n",
    "        if label == 1:\n",
    "            matching_detections.append(d)\n",
    "        elif label == 44:\n",
    "            obstacle.append(d)\n",
    "    \n",
    "\n",
    "    closest_obstacle = get_closest_object(obstacle)\n",
    "    \n",
    "    if closest_obstacle:\n",
    "        det = closest_obstacle\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "        \n",
    "        if depth < 1500:\n",
    "            if middle_width < 0.5:\n",
    "                obstacle_avoid_right()\n",
    "            elif middle_width < 0.5:\n",
    "                obstacle_avoid_left()\n",
    "        \n",
    "    \n",
    "    closest_human = get_closest_object(matching_detections)\n",
    "\n",
    "    if closest_human:\n",
    "        det = closest_human\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "\n",
    "        if middle_width < 0.4:\n",
    "            robot.left(0.5)\n",
    "        elif middle_width > 0.6:\n",
    "            robot.right(0.5)\n",
    "        else:\n",
    "            if depth < 600:\n",
    "                robot.backward(0.3)\n",
    "            elif 800 > depth > 600:\n",
    "                robot.stop()\n",
    "            else:\n",
    "                robot.forward(0.25)\n",
    "          \n",
    "        \n",
    "    else:\n",
    "        if depth_min > 300:\n",
    "            robot.forward(0.4)\n",
    "        else:\n",
    "            robot.backward(1)\n",
    "            robot.left(0.4)\n",
    "            time.sleep(0.5)\n",
    "            robot.stop()\n",
    "          \n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---FEAR---\n",
    "def processing(change):\n",
    "    \n",
    "    depth_track = []\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == 1]\n",
    "\n",
    "    \n",
    "    \n",
    "    if matching_detections:\n",
    "        for detection in matching_detections:\n",
    "            det = detection\n",
    "            bbox = det['bbox']\n",
    "            create_blue_box(image, bbox)\n",
    "            x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "            depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "    \n",
    "            if middle_width < 0.4:\n",
    "                robot.left(1)\n",
    "                robot.stop()\n",
    "            elif middle_width > 0.6:\n",
    "                robot.right(1)\n",
    "                robot.stop()\n",
    "            else:\n",
    "                if 600 > depth > 0:\n",
    "                    robot.left(1)\n",
    "                    time.sleep(1)\n",
    "                    robot.forward(0.7)\n",
    "                    time.sleep(1)\n",
    "                    robot.right(1)\n",
    "                    time.sleep(1)         \n",
    "                    robot.stop()\n",
    "          \n",
    "          \n",
    "          \n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---AGRESSION---\n",
    "def processing(change):\n",
    "    \n",
    "    depth_track = []\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    #\n",
    "    \n",
    "    #   AVOID BOTTLES  \n",
    "    matching_detections = []\n",
    "    obstacle = []\n",
    "    \n",
    "    for d in detections[0]:\n",
    "        label = d['label']\n",
    "        if label == 1:\n",
    "            matching_detections.append(d)\n",
    "        elif label == 44:\n",
    "            obstacle.append(d)\n",
    "    \n",
    "\n",
    "    closest_obstacle = get_closest_object(obstacle)\n",
    "    \n",
    "    if closest_obstacle:\n",
    "        det = closest_obstacle\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "        \n",
    "        if depth < 1500:\n",
    "            if middle_width < 0.5:\n",
    "                obstacle_avoid_right()\n",
    "            elif middle_width < 0.5:\n",
    "                obstacle_avoid_left()\n",
    "        \n",
    "    \n",
    "    closest_human = get_closest_object(matching_detections)\n",
    "\n",
    "    if closest_human:\n",
    "        det = closest_human\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "\n",
    "        if middle_width < 0.4:\n",
    "            robot.left(0.5)\n",
    "        elif middle_width > 0.6:\n",
    "            robot.right(0.5)\n",
    "        else:\n",
    "            if 600 > depth > 0:\n",
    "                robot.forward(8)\n",
    "                time.sleep(1.5)\n",
    "                robot.stop()\n",
    "            else:\n",
    "                robot.forward(0.5)\n",
    "          \n",
    "          \n",
    "          \n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')    \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Love\n",
    "\n",
    "def processing(change):\n",
    "    \n",
    "    depth_track = []\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "robot.stop()\n",
    "     \n",
    "    matching_detections = []\n",
    "    \n",
    "    for d in detections[0]:\n",
    "        label = d['label']\n",
    "        if label == 1:\n",
    "            matching_detections.append(d)\n",
    "    \n",
    "    \n",
    "    closest_human = get_closest_object(matching_detections)\n",
    "\n",
    "    if closest_human:\n",
    "        det = closest_human\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "        print(f'Person Depth: {depth}')\n",
    "\n",
    "        if middle_width < 0.4:\n",
    "            print(f'Person left: {middle_width}')\n",
    "            robot.left(0.5)\n",
    "        elif middle_width > 0.6:\n",
    "            print(f'Person right: {middle_width}')\n",
    "            robot.right(0.5)\n",
    "        else:\n",
    "            if depth < 600:\n",
    "                robot.backward(0.5)\n",
    "            elif 1000 > depth > 600:\n",
    "                robot.stop()\n",
    "            else:\n",
    "                robot.forward(0.5)\n",
    "          \n",
    "          \n",
    "          \n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curious\n",
    "\n",
    "def processing(change):\n",
    "    \n",
    "    depth_track = []\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    #\n",
    "     \n",
    "    matching_detections = []\n",
    "    \n",
    "    for d in detections[0]:\n",
    "        label = d['label']\n",
    "        if label == 1:\n",
    "            matching_detections.append(d)\n",
    "    \n",
    "    \n",
    "    closest_human = get_closest_object(matching_detections)\n",
    "\n",
    "    if closest_human:\n",
    "        det = closest_human\n",
    "        bbox = det['bbox']\n",
    "        create_blue_box(image, bbox)\n",
    "        x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "        depth = get_depth_from_data(camera.depth_image_, x , y)\n",
    "        print(f'Person Depth: {depth}')\n",
    "\n",
    "        if middle_width < 0.4:\n",
    "            print(f'Person left: {middle_width}')\n",
    "            robot.left(0.5)\n",
    "        elif middle_width > 0.6:\n",
    "            print(f'Person right: {middle_width}')\n",
    "            robot.right(0.5)\n",
    "        else:\n",
    "            if not depth:\n",
    "                return\n",
    "            if depth > 1500:\n",
    "                robot.stop()\n",
    "            elif depth < 1500:\n",
    "                robot.forward(0.25)\n",
    "                if depth < 500:\n",
    "                    robot.stop()\n",
    "          \n",
    "          \n",
    "          \n",
    "    image_widget.value = bgr8_to_jpeg(image)      \n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget,]),\n",
    "    label_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.stop()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Testing---\n",
    "\n",
    "def test_get_x_y():\n",
    "    width = 640\n",
    "    height = 480\n",
    "    bbox = [0.2,0.1,0.7,0.9]\n",
    "    real_middle_width =round(((0.7-0.2)/2)+0.2,1)\n",
    "    real_middle_height = round(((0.9-0.1)/2)+0.1,1)\n",
    "    real_y = width*real_middle_width\n",
    "    real_x = height*real_middle_height\n",
    "    \n",
    "    x, y, middle_width, middle_height = get_x_y(bbox)\n",
    "    correct=0\n",
    "    if real_middle_height == middle_height:\n",
    "        print('middle height value correct')\n",
    "        correct = correct+1\n",
    "    else:\n",
    "        print('middle height value incorrect')\n",
    "        \n",
    "    if real_middle_width == middle_width:\n",
    "        print('middle width value correct')\n",
    "        correct = correct+1\n",
    "    else:\n",
    "        print('middle width value incorrect')\n",
    "        \n",
    "    if real_y == y:\n",
    "        print('relative width value correct')\n",
    "        correct = correct+1\n",
    "    else:\n",
    "        print('relative width value incorrect')\n",
    "        \n",
    "    if real_x == x:\n",
    "        print('relative height value correct')\n",
    "        correct = correct+1\n",
    "    else:\n",
    "        print('relative height value incorrect')\n",
    "        \n",
    "    print(str(correct)+'/4 were correct')\n",
    "    \n",
    "    \n",
    "def test_part_b():\n",
    "    \n",
    "    width = 640\n",
    "    height = 480\n",
    "    bbox_left = [0.2,0.1,0.5,0.8]\n",
    "    bbox_right = [0.6,0.6,0.8,0.8]\n",
    "    bbox_middle = [0.2,0.1,0.8,0.8]\n",
    "    x, y, middle_width_left, middle_height = get_x_y(bbox_left)\n",
    "    x, y, middle_width_right, middle_height = get_x_y(bbox_right)\n",
    "    x, y, middle_width_middle, middle_height = get_x_y(bbox_middle)    \n",
    "    \n",
    "    tests=[middle_width_left,middle_width_right,middle_width_middle]\n",
    "    results=[]\n",
    "    print('Expected: left, rigth, middle')\n",
    "    print('Got:')\n",
    "    for middle_width in tests:\n",
    "        if middle_width < 0.4:\n",
    "            print('bbox centre to the left')\n",
    "        elif middle_width > 0.6:\n",
    "            print('bbox cetre to the right')\n",
    "        else:\n",
    "            print('bbox centre in the middle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "middle height value correct\n",
      "middle width value correct\n",
      "relative width value correct\n",
      "relative height value correct\n",
      "4/4 were correct\n"
     ]
    }
   ],
   "source": [
    "test_get_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: left, rigth, middle\n",
      "Got:\n",
      "bbox centre to the left\n",
      "bbox cetre to the right\n",
      "bbox centre in the middle\n"
     ]
    }
   ],
   "source": [
    "test_part_b()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
